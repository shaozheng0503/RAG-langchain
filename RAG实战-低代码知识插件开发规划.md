# RAGå®æˆ˜-ä½ä»£ç çŸ¥è¯†æ’ä»¶å¼€å‘è§„åˆ’

## 1 é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®èƒŒæ™¯
æœ¬é¡¹ç›®å°† RAGå®æˆ˜ çš„æ ¸å¿ƒæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ä¸ä½ä»£ç "çŸ¥è¯†æ’ä»¶"æ¦‚å¿µç›¸ç»“åˆï¼Œä¸º Notionã€Confluenceã€é£ä¹¦å¤šç»´è¡¨ç­‰åä½œå¹³å°æä¾›ä¸€é”®é—®ç­”æ’ä»¶ã€‚é€šè¿‡æ•´åˆ notebooks ç›®å½•ä¸­çš„ RAG æŠ€æœ¯å®ç°ï¼ŒåŒ…æ‹¬å¤šæŸ¥è¯¢è½¬æ¢ã€æ™ºèƒ½è·¯ç”±ã€é«˜çº§ç´¢å¼•å’Œé‡æ–°æ’åºç­‰æ ¸å¿ƒåŠŸèƒ½ï¼Œæ„å»ºå®Œæ•´çš„æ™ºèƒ½åŒ–çŸ¥è¯†æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿã€‚

### 1.2 æ ¸å¿ƒä»·å€¼ä¸»å¼ 
- **ä½ä»£ç é›†æˆ**ï¼šæä¾›ç®€å•çš„é…ç½®ç•Œé¢ï¼Œç”¨æˆ·æ— éœ€ç¼–å†™ä»£ç å³å¯é›†æˆåˆ°ç°æœ‰å¹³å°
- **æ™ºèƒ½é—®ç­”**ï¼šåŸºäº notebooks ä¸­çš„ RAG æŠ€æœ¯æ ˆï¼Œå®ç°å‡†ç¡®ã€ç›¸å…³çš„çŸ¥è¯†æ£€ç´¢å’Œå›ç­”
- **è·¨å¹³å°å…¼å®¹**ï¼šæ”¯æŒå¤šç§ä¸»æµåä½œå¹³å°ï¼Œç»Ÿä¸€çš„çŸ¥è¯†ç®¡ç†ä½“éªŒ
- **å®æ—¶åŒæ­¥**ï¼šè‡ªåŠ¨åŒæ­¥å¹³å°å†…å®¹æ›´æ–°ï¼Œä¿æŒçŸ¥è¯†åº“çš„æ—¶æ•ˆæ€§
- **æŠ€æœ¯å…ˆè¿›æ€§**ï¼šæ•´åˆæœ€æ–°çš„ RAG æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤šæŸ¥è¯¢ç”Ÿæˆã€æ™ºèƒ½è·¯ç”±ã€é«˜çº§ç´¢å¼•ç­‰

## 2 æŠ€æœ¯æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å‰ç«¯ç•Œé¢å±‚     â”‚    â”‚   å¹³å°é€‚é…å±‚     â”‚    â”‚   RAG å¼•æ“å±‚    â”‚
â”‚  (React/TS)     â”‚â—„â”€â”€â–ºâ”‚  (Platform      â”‚â—„â”€â”€â–ºâ”‚  (LangChain     â”‚
â”‚                 â”‚    â”‚   Adapters)     â”‚    â”‚   + OpenAI)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å†…å®¹åŒæ­¥å±‚     â”‚    â”‚   æ•°æ®å­˜å‚¨å±‚     â”‚    â”‚   ç›‘æ§è¿ç»´å±‚     â”‚
â”‚  (Sync Engine)  â”‚    â”‚  (PostgreSQL    â”‚    â”‚  (Prometheus    â”‚
â”‚                 â”‚    â”‚   + Redis)      â”‚    â”‚   + Grafana)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 2.2.1 RAG å¼•æ“æ ¸å¿ƒ (åŸºäº notebooks æŠ€æœ¯æ ˆ)
åŸºäº notebooks ç›®å½•ä¸­çš„ RAG å®ç°ï¼Œæ„å»ºæ ¸å¿ƒæ£€ç´¢å¢å¼ºç”Ÿæˆå¼•æ“ã€‚è¯¥å¼•æ“æ•´åˆäº†ä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯ï¼š

- **[1]_rag_setup_overview_chinese.py**: ç¯å¢ƒè®¾ç½®å’ŒåŸºç¡€é…ç½®
- **[2]_rag_with_multi_query_chinese.py**: å¤šæŸ¥è¯¢è½¬æ¢æŠ€æœ¯
- **[3]_rag_routing_and_query_construction_chinese.py**: æ™ºèƒ½è·¯ç”±å’ŒæŸ¥è¯¢æ„å»º
- **[4]_rag_indexing_and_advanced_retrieval_chinese.py**: é«˜çº§ç´¢å¼•å’Œæ£€ç´¢
- **[5]_rag_retrieval_and_reranking_chinese.py**: æ£€ç´¢é‡æ–°æ’åºä¼˜åŒ–

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGå®æˆ˜ æ ¸å¿ƒå¼•æ“
åŸºäº notebooks ç›®å½•ä¸­çš„ RAG æŠ€æœ¯å®ç°
æ•´åˆäº†å¤šæŸ¥è¯¢è½¬æ¢ã€æ™ºèƒ½è·¯ç”±ã€é«˜çº§ç´¢å¼•å’Œé‡æ–°æ’åºç­‰æ ¸å¿ƒæŠ€æœ¯
"""

import os
from typing import List, Dict, Any
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Pinecone, ChromaDB
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

class RAGCoreEngine:
    """RAG æ ¸å¿ƒå¼•æ“ï¼Œæ•´åˆ notebooks ä¸­çš„æŠ€æœ¯å®ç°"""
    
    def __init__(self):
        """åˆå§‹åŒ– RAG å¼•æ“"""
        load_dotenv()
        self._setup_environment()
        self._initialize_components()
    
    def _setup_environment(self):
        """ç¯å¢ƒè®¾ç½®ï¼ŒåŸºäº [1]_rag_setup_overview_chinese.py"""
        # LangSmith è®¾ç½®
        os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2', 'false')
        os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT', '')
        os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY', '')
        
        # API å¯†é’¥
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.pinecone_api_key = os.getenv('PINECONE_API_KEY')
        self.pinecone_host = os.getenv('PINECONE_API_HOST')
        self.index_name = os.getenv('PINECONE_INDEX_NAME')
        self.cohere_api_key = os.getenv('COHERE_API_KEY')
    
    def _initialize_components(self):
        """åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶"""
        # åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            openai_api_key=self.openai_api_key
        )
        
        # LLM æ¨¡å‹
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.1,
            openai_api_key=self.openai_api_key
        )
        
        # å‘é‡æ•°æ®åº“
        self._setup_vector_store()
    
    def _setup_vector_store(self):
        """è®¾ç½®å‘é‡æ•°æ®åº“ï¼Œæ”¯æŒ Pinecone å’Œ ChromaDB"""
        if self.pinecone_api_key:
            import pinecone
            pinecone.init(api_key=self.pinecone_api_key, environment=self.pinecone_host)
            self.vector_store = Pinecone.from_existing_index(
                index_name=self.index_name,
                embedding=self.embeddings
            )
        else:
            # æœ¬åœ° ChromaDB ä½œä¸ºå¤‡é€‰
            self.vector_store = ChromaDB(
                embedding_function=self.embeddings,
                persist_directory="./chroma_db"
            )
    
    def process_documents(self, documents: List[str]) -> None:
        """å¤„ç†æ–‡æ¡£å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        # æ–‡æ¡£åˆ†å—ï¼ŒåŸºäº [4]_rag_indexing_and_advanced_retrieval_chinese.py
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
        
        chunks = text_splitter.split_documents(documents)
        self.vector_store.add_documents(chunks)
    
    def query(self, question: str, use_reranking: bool = True) -> str:
        """æ‰§è¡Œ RAG æŸ¥è¯¢"""
        # åŸºç¡€æ£€ç´¢
        retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}
        )
        
        if use_reranking:
            # é‡æ–°æ’åºï¼ŒåŸºäº [5]_rag_retrieval_and_reranking_chinese.py
            retriever = self._apply_reranking(retriever)
        
        # æ„å»º RAG é“¾
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True
        )
        
        result = qa_chain({"query": question})
        return result["result"]
    
    def _apply_reranking(self, retriever):
        """åº”ç”¨é‡æ–°æ’åºï¼Œæå‡æ£€ç´¢è´¨é‡"""
        if self.cohere_api_key:
            from langchain.retrievers import ContextualCompressionRetriever
            from langchain.retrievers.document_compressors import CohereRerank
            
            compressor = CohereRerank(
                cohere_api_key=self.cohere_api_key,
                top_n=3
            )
            return ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=retriever
            )
        return retriever

#### 2.2.5 å®Œæ•´ RAG é›†æˆç¤ºä¾‹
ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•å°†ä¸Šè¿°æ‰€æœ‰ç»„ä»¶æ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„ RAG ç³»ç»Ÿï¼š

```python
class IntegratedRAGSystem:
    """å®Œæ•´çš„ RAG é›†æˆç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–é›†æˆç³»ç»Ÿ"""
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.rag_engine = RAGCoreEngine()
        self.multi_query_engine = MultiQueryEngine(self.rag_engine.llm)
        self.smart_router = SmartRouter()
        self.advanced_indexer = AdvancedIndexingEngine(
            self.rag_engine.embeddings, 
            self.rag_engine.llm
        )
        
        # ç³»ç»Ÿé…ç½®
        self.config = {
            'enable_multi_query': True,
            'enable_smart_routing': True,
            'enable_reranking': True,
            'cache_results': True
        }
    
    def process_query(self, user_query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´æµç¨‹"""
        if context is None:
            context = {}
        
        # 1. æ™ºèƒ½è·¯ç”±
        if self.config['enable_smart_routing']:
            routing_strategy = self.smart_router.route_query(user_query, context)
            print(f"æŸ¥è¯¢è·¯ç”±ç­–ç•¥: {routing_strategy}")
        else:
            routing_strategy = {'strategy': 'default_search'}
        
        # 2. å¤šæŸ¥è¯¢ç”Ÿæˆ
        if self.config['enable_multi_query']:
            queries = self.multi_query_engine.generate_multiple_queries(
                user_query, 
                routing_strategy.get('num_queries', 3)
            )
            print(f"ç”Ÿæˆçš„å¤šæŸ¥è¯¢: {queries}")
        else:
            queries = [user_query]
        
        # 3. æ‰§è¡Œæ£€ç´¢
        all_documents = []
        for query in queries:
            docs = self._retrieve_documents(query, routing_strategy)
            all_documents.extend(docs)
        
        # 4. æ–‡æ¡£å»é‡å’Œæ’åº
        unique_docs = self.multi_query_engine._deduplicate_documents(all_documents)
        
        # 5. é‡æ–°æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config['enable_reranking']:
            final_docs = self._apply_final_reranking(unique_docs, user_query)
        else:
            final_docs = unique_docs[:routing_strategy.get('top_k', 3)]
        
        # 6. ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        answer = self._generate_answer(user_query, final_docs, context)
        
        # 7. æ„å»ºå“åº”
        response = {
            'answer': answer,
            'source_documents': final_docs,
            'routing_strategy': routing_strategy,
            'generated_queries': queries,
            'total_documents_retrieved': len(all_documents),
            'final_documents_used': len(final_docs)
        }
        
        return response
    
    def _retrieve_documents(self, query: str, strategy: Dict[str, Any]) -> List[Any]:
        """æ ¹æ®ç­–ç•¥æ£€ç´¢æ–‡æ¡£"""
        # æ ¹æ®è·¯ç”±ç­–ç•¥è°ƒæ•´æ£€ç´¢å‚æ•°
        retriever = self.rag_engine.vector_store.as_retriever(
            search_type=strategy.get('retriever_type', 'similarity'),
            search_kwargs={"k": strategy.get('top_k', 5)}
        )
        
        return retriever.get_relevant_documents(query)
    
    def _apply_final_reranking(self, documents: List[Any], query: str) -> List[Any]:
        """åº”ç”¨æœ€ç»ˆé‡æ–°æ’åº"""
        if not documents:
            return []
        
        # ä½¿ç”¨ Cohere é‡æ–°æ’åº
        if self.rag_engine.cohere_api_key:
            try:
                import cohere
                co = cohere.Client(self.rag_engine.cohere_api_key)
                
                # å‡†å¤‡é‡æ–°æ’åºæ•°æ®
                texts = [doc.page_content for doc in documents]
                
                response = co.rerank(
                    query=query,
                    documents=texts,
                    top_n=min(3, len(texts)),
                    model='rerank-multilingual-v2.0'
                )
                
                # æ ¹æ®é‡æ–°æ’åºç»“æœé‡æ–°æ’åˆ—æ–‡æ¡£
                reranked_docs = []
                for result in response.results:
                    doc_index = result.index
                    reranked_docs.append(documents[doc_index])
                
                return reranked_docs
            except Exception as e:
                print(f"é‡æ–°æ’åºå¤±è´¥: {e}")
                return documents[:3]
        else:
            return documents[:3]
    
    def _generate_answer(self, query: str, documents: List[Any], context: Dict[str, Any]) -> str:
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        if not documents:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = "\n\n".join([doc.page_content for doc in documents])
        
        # æ„å»ºæç¤ºæ¨¡æ¿
        prompt = PromptTemplate(
            input_variables=["query", "context", "user_context"],
            template="""
            åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
            
            ç”¨æˆ·é—®é¢˜ï¼š{query}
            ç›¸å…³ä¿¡æ¯ï¼š{context}
            ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼š{user_context}
            
            è¦æ±‚ï¼š
            1. å›ç­”è¦å‡†ç¡®ã€ç›¸å…³ã€æœ‰ç”¨
            2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜
            3. ä½¿ç”¨ä¸­æ–‡å›ç­”
            4. ä¿æŒå›ç­”çš„ç®€æ´æ€§
            
            å›ç­”ï¼š
            """
        )
        
        # ç”Ÿæˆç­”æ¡ˆ
        response = self.rag_engine.llm.invoke(
            prompt.format(
                query=query,
                context=context_text,
                user_context=context.get('user_info', 'æ— ')
            )
        )
        
        return response.content
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'rag_engine_status': 'active',
            'vector_store_status': 'connected',
            'index_statistics': self.advanced_indexer.get_index_statistics(),
            'configuration': self.config,
            'last_update': datetime.now().isoformat()
        }
    
    def update_configuration(self, new_config: Dict[str, Any]) -> None:
        """æ›´æ–°ç³»ç»Ÿé…ç½®"""
        self.config.update(new_config)
        print(f"é…ç½®å·²æ›´æ–°: {self.config}")
    
    def optimize_system(self, query_logs: List[Dict[str, Any]]) -> None:
        """åŸºäºæŸ¥è¯¢æ—¥å¿—ä¼˜åŒ–ç³»ç»Ÿ"""
        # åˆ†ææŸ¥è¯¢æ¨¡å¼
        query_patterns = [log['query'] for log in query_logs]
        
        # ä¼˜åŒ–ç´¢å¼•
        self.advanced_indexer.optimize_index(query_patterns)
        
        # ä¼˜åŒ–è·¯ç”±ç­–ç•¥
        self._optimize_routing_strategy(query_logs)
        
        print("ç³»ç»Ÿä¼˜åŒ–å®Œæˆ")
    
    def _optimize_routing_strategy(self, query_logs: List[Dict[str, Any]]) -> None:
        """ä¼˜åŒ–è·¯ç”±ç­–ç•¥"""
        # åˆ†ææŸ¥è¯¢ç±»å‹åˆ†å¸ƒ
        query_types = {}
        for log in query_logs:
            query_type = self.smart_router._classify_query(log['query'])
            query_types[query_type] = query_types.get(query_type, 0) + 1
        
        print(f"æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ: {query_types}")
        
        # æ ¹æ®åˆ†å¸ƒè°ƒæ•´ç­–ç•¥æƒé‡
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ä¼˜åŒ–é€»è¾‘
```

#### 2.2.2 å¤šæŸ¥è¯¢è½¬æ¢å¼•æ“ (åŸºäº [2]_rag_with_multi_query_chinese.py)
å¤šæŸ¥è¯¢è½¬æ¢æŠ€æœ¯èƒ½å¤Ÿä»ä¸åŒè§’åº¦æ¢ç´¢ç”¨æˆ·é—®é¢˜ï¼Œæå‡æ£€ç´¢çš„è¦†ç›–èŒƒå›´å’Œå‡†ç¡®æ€§ã€‚è¯¥å¼•æ“åŸºäº notebooks ä¸­çš„å¤šæŸ¥è¯¢å®ç°ï¼Œæ”¯æŒåŠ¨æ€æŸ¥è¯¢ç”Ÿæˆå’Œæ™ºèƒ½æ–‡æ¡£å»é‡ã€‚

```python
class MultiQueryEngine:
    """å¤šæŸ¥è¯¢è½¬æ¢å¼•æ“ï¼Œæå‡æ£€ç´¢è¦†ç›–èŒƒå›´"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def generate_multiple_queries(self, original_query: str, num_queries: int = 3) -> List[str]:
        """ç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢"""
        prompt = PromptTemplate(
            input_variables=["question", "num_queries"],
            template="""
            åŸºäºä»¥ä¸‹é—®é¢˜ï¼Œç”Ÿæˆ {num_queries} ä¸ªä¸åŒçš„ç›¸å…³æŸ¥è¯¢ï¼Œç”¨äºä¿¡æ¯æ£€ç´¢ï¼š
            åŸé—®é¢˜ï¼š{question}
            
            è¦æ±‚ï¼š
            1. æ¯ä¸ªæŸ¥è¯¢éƒ½åº”è¯¥ä»ä¸åŒè§’åº¦æ¢ç´¢åŸé—®é¢˜
            2. ä½¿ç”¨åŒä¹‰è¯å’Œç›¸å…³æ¦‚å¿µ
            3. ä¿æŒæŸ¥è¯¢çš„ç®€æ´æ€§å’Œç›¸å…³æ€§
            
            è¯·ç›´æ¥è¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªï¼š
            """
        )
        
        response = self.llm.invoke(
            prompt.format(question=original_query, num_queries=num_queries)
        )
        
        queries = [q.strip() for q in response.content.split('\n') if q.strip()]
        return queries[:num_queries]
    
    def execute_multi_query_retrieval(self, queries: List[str], retriever) -> List[Any]:
        """æ‰§è¡Œå¤šæŸ¥è¯¢æ£€ç´¢"""
        all_docs = []
        for query in queries:
            docs = retriever.get_relevant_documents(query)
            all_docs.extend(docs)
        
        # å»é‡å’Œæ’åº
        unique_docs = self._deduplicate_documents(all_docs)
        return unique_docs
    
    def _deduplicate_documents(self, documents: List[Any]) -> List[Any]:
        """æ–‡æ¡£å»é‡"""
        seen = set()
        unique_docs = []
        
        for doc in documents:
            doc_hash = hash(doc.page_content[:100])  # åŸºäºå†…å®¹å‰100å­—ç¬¦å»é‡
            if doc_hash not in seen:
                seen.add(doc_hash)
                unique_docs.append(doc)
        
        return unique_docs
    
    def optimize_query_strategy(self, original_query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼˜åŒ–æŸ¥è¯¢ç­–ç•¥ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´æŸ¥è¯¢æ•°é‡"""
        # æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦è°ƒæ•´æŸ¥è¯¢æ•°é‡
        if len(original_query) > 50:
            num_queries = 4
        elif len(original_query) > 20:
            num_queries = 3
        else:
            num_queries = 2
        
        return {
            'num_queries': num_queries,
            'strategy': 'adaptive',
            'context_aware': True
        }
```

#### 2.2.3 æ™ºèƒ½è·¯ç”±å¼•æ“ (åŸºäº [3]_rag_routing_and_query_construction_chinese.py)
æ™ºèƒ½è·¯ç”±å¼•æ“èƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„å¤„ç†ç­–ç•¥ï¼Œæå‡æ£€ç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥å¼•æ“åŸºäº notebooks ä¸­çš„è·¯ç”±å’ŒæŸ¥è¯¢æ„å»ºæŠ€æœ¯ï¼Œæ”¯æŒå¤šç§æŸ¥è¯¢ç±»å‹çš„æ™ºèƒ½è¯†åˆ«å’Œç­–ç•¥é€‰æ‹©ã€‚

```python
class SmartRouter:
    """æ™ºèƒ½è·¯ç”±å¼•æ“ï¼Œæ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©æœ€ä½³å¤„ç†ç­–ç•¥"""
    
    def __init__(self):
        self.route_patterns = {
            'factual': self._route_to_factual_search,
            'analytical': self._route_to_analytical_search,
            'comparative': self._route_to_comparative_search,
            'procedural': self._route_to_procedural_search
        }
        
        # åŠ è½½é¢„è®­ç»ƒçš„åˆ†ç±»æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        self.classifier = self._load_query_classifier()
    
    def route_query(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """è·¯ç”±æŸ¥è¯¢åˆ°åˆé€‚çš„å¤„ç†ç­–ç•¥"""
        query_type = self._classify_query(query)
        route_function = self.route_patterns.get(query_type, self._route_to_default)
        
        # è®°å½•è·¯ç”±å†³ç­–ç”¨äºåç»­ä¼˜åŒ–
        self._log_routing_decision(query, query_type, context)
        
        return route_function(query, context)
    
    def _classify_query(self, query: str) -> str:
        """æŸ¥è¯¢åˆ†ç±»"""
        # åŸºäºå…³é”®è¯å’Œæ¨¡å¼è¯†åˆ«æŸ¥è¯¢ç±»å‹
        if any(word in query.lower() for word in ['æ˜¯ä»€ä¹ˆ', 'å®šä¹‰', 'æ¦‚å¿µ', 'è§£é‡Š']):
            return 'factual'
        elif any(word in query.lower() for word in ['åˆ†æ', 'åŸå› ', 'å½±å“', 'ä¸ºä»€ä¹ˆ']):
            return 'analytical'
        elif any(word in query.lower() for word in ['æ¯”è¾ƒ', 'åŒºåˆ«', 'vs', 'å¯¹æ¯”']):
            return 'comparative'
        elif any(word in query.lower() for word in ['å¦‚ä½•', 'æ­¥éª¤', 'æ–¹æ³•', 'æµç¨‹']):
            return 'procedural'
        else:
            return 'factual'
    
    def _route_to_factual_search(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """äº‹å®æ€§æŸ¥è¯¢è·¯ç”±"""
        return {
            'strategy': 'factual_search',
            'retriever_type': 'similarity',
            'chunk_size': 1000,
            'top_k': 3,
            'reranking': True,
            'confidence_threshold': 0.8
        }
    
    def _route_to_analytical_search(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ€§æŸ¥è¯¢è·¯ç”±"""
        return {
            'strategy': 'analytical_search',
            'retriever_type': 'mmr',  # æœ€å¤§è¾¹é™…ç›¸å…³æ€§
            'chunk_size': 1500,
            'top_k': 5,
            'reranking': True,
            'confidence_threshold': 0.7
        }
    
    def _route_to_comparative_search(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒæ€§æŸ¥è¯¢è·¯ç”±"""
        return {
            'strategy': 'comparative_search',
            'retriever_type': 'similarity',
            'chunk_size': 2000,
            'top_k': 7,
            'reranking': True,
            'confidence_threshold': 0.75
        }
    
    def _route_to_procedural_search(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ç¨‹åºæ€§æŸ¥è¯¢è·¯ç”±"""
        return {
            'strategy': 'procedural_search',
            'retriever_type': 'similarity',
            'chunk_size': 800,
            'top_k': 4,
            'reranking': False,
            'confidence_threshold': 0.9
        }
    
    def _route_to_default(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """é»˜è®¤è·¯ç”±"""
        return {
            'strategy': 'default_search',
            'retriever_type': 'similarity',
            'chunk_size': 1000,
            'top_k': 3,
            'reranking': True,
            'confidence_threshold': 0.8
        }
    
    def _load_query_classifier(self):
        """åŠ è½½æŸ¥è¯¢åˆ†ç±»å™¨ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        try:
            # è¿™é‡Œå¯ä»¥åŠ è½½é¢„è®­ç»ƒçš„æŸ¥è¯¢åˆ†ç±»æ¨¡å‹
            # ä¾‹å¦‚ï¼šBERTã€RoBERTa ç­‰
            return None
        except Exception as e:
            print(f"æ— æ³•åŠ è½½æŸ¥è¯¢åˆ†ç±»å™¨: {e}")
            return None
    
    def _log_routing_decision(self, query: str, query_type: str, context: Dict[str, Any]):
        """è®°å½•è·¯ç”±å†³ç­–ç”¨äºåç»­ä¼˜åŒ–"""
        # è®°å½•åˆ°æ—¥å¿—ç³»ç»Ÿæˆ–æ•°æ®åº“
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'query_type': query_type,
            'context': context,
            'user_id': context.get('user_id', 'anonymous')
        }
        # è¿™é‡Œå¯ä»¥é›†æˆåˆ°æ—¥å¿—ç³»ç»Ÿ
        print(f"è·¯ç”±å†³ç­–: {log_entry}")
```

#### 2.2.4 é«˜çº§ç´¢å¼•å¼•æ“ (åŸºäº [4]_rag_indexing_and_advanced_retrieval_chinese.py)
é«˜çº§ç´¢å¼•å¼•æ“æ”¯æŒå¤šè¡¨ç¤ºç´¢å¼•ã€æ‘˜è¦å­˜å‚¨å’Œæ™ºèƒ½æ–‡æ¡£åˆ†å—ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥å¼•æ“åŸºäº notebooks ä¸­çš„é«˜çº§ç´¢å¼•å’Œæ£€ç´¢æŠ€æœ¯ï¼Œå®ç°äº†å¤šç§ç´¢å¼•ç­–ç•¥çš„æ™ºèƒ½ç»„åˆã€‚

```python
class AdvancedIndexingEngine:
    """é«˜çº§ç´¢å¼•å¼•æ“ï¼Œæ”¯æŒå¤šè¡¨ç¤ºç´¢å¼•å’Œæ‘˜è¦å­˜å‚¨"""
    
    def __init__(self, embeddings, llm):
        self.embeddings = embeddings
        self.llm = llm
        self.index_cache = {}  # ç´¢å¼•ç¼“å­˜
    
    def create_multi_representation_index(self, documents: List[str]) -> Dict[str, Any]:
        """åˆ›å»ºå¤šè¡¨ç¤ºç´¢å¼•"""
        # 1. åŸå§‹æ–‡æ¡£ç´¢å¼•
        original_index = self._create_original_index(documents)
        
        # 2. æ‘˜è¦ç´¢å¼•
        summary_index = self._create_summary_index(documents)
        
        # 3. å…³é”®è¯ç´¢å¼•
        keyword_index = self._create_keyword_index(documents)
        
        # 4. è¯­ä¹‰ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
        semantic_index = self._create_semantic_index(documents)
        
        return {
            'original': original_index,
            'summary': summary_index,
            'keywords': keyword_index,
            'semantic': semantic_index,
            'metadata': self._extract_metadata(documents)
        }
    
    def _create_original_index(self, documents: List[str]) -> Any:
        """åˆ›å»ºåŸå§‹æ–‡æ¡£ç´¢å¼•"""
        # ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼ŒåŸºäº notebooks ä¸­çš„æœ€ä½³å®è·µ
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
        
        chunks = text_splitter.split_documents(documents)
        embeddings = self.embeddings.embed_documents([chunk.page_content for chunk in chunks])
        
        # ç¼“å­˜ç´¢å¼•ç»“æœ
        self.index_cache['original'] = {
            'chunks': chunks,
            'embeddings': embeddings
        }
        
        return embeddings
    
    def _create_summary_index(self, documents: List[str]) -> Any:
        """åˆ›å»ºæ‘˜è¦ç´¢å¼•"""
        summaries = []
        for doc in documents:
            summary = self._generate_summary(doc)
            summaries.append(summary)
        
        embeddings = self.embeddings.embed_documents(summaries)
        
        # ç¼“å­˜æ‘˜è¦ç´¢å¼•
        self.index_cache['summary'] = {
            'summaries': summaries,
            'embeddings': embeddings
        }
        
        return embeddings
    
    def _create_keyword_index(self, documents: List[str]) -> Any:
        """åˆ›å»ºå…³é”®è¯ç´¢å¼•"""
        keywords_list = []
        for doc in documents:
            keywords = self._extract_keywords(doc)
            keywords_list.append(' '.join(keywords))
        
        embeddings = self.embeddings.embed_documents(keywords_list)
        
        # ç¼“å­˜å…³é”®è¯ç´¢å¼•
        self.index_cache['keywords'] = {
            'keywords': keywords_list,
            'embeddings': embeddings
        }
        
        return embeddings
    
    def _create_semantic_index(self, documents: List[str]) -> Any:
        """åˆ›å»ºè¯­ä¹‰ç´¢å¼•ï¼ˆåŸºäºæ–‡æ¡£ä¸»é¢˜å’Œæ¦‚å¿µï¼‰"""
        semantic_representations = []
        for doc in documents:
            semantic_rep = self._extract_semantic_representation(doc)
            semantic_representations.append(semantic_rep)
        
        embeddings = self.embeddings.embed_documents(semantic_representations)
        
        # ç¼“å­˜è¯­ä¹‰ç´¢å¼•
        self.index_cache['semantic'] = {
            'representations': semantic_representations,
            'embeddings': embeddings
        }
        
        return embeddings
    
    def _generate_summary(self, document: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        prompt = PromptTemplate(
            input_variables=["document"],
            template="è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š\n\n{document}"
        )
        
        response = self.llm.invoke(prompt.format(document=document[:2000]))
        return response.content
    
    def _extract_keywords(self, document: str) -> List[str]:
        """æå–æ–‡æ¡£å…³é”®è¯"""
        prompt = PromptTemplate(
            input_variables=["document"],
            template="è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–5-8ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼š\n\n{document}"
        )
        
        response = self.llm.invoke(prompt.format(document=document[:2000]))
        keywords = [kw.strip() for kw in response.content.split(',')]
        return keywords[:8]
    
    def _extract_semantic_representation(self, document: str) -> str:
        """æå–æ–‡æ¡£çš„è¯­ä¹‰è¡¨ç¤º"""
        prompt = PromptTemplate(
            input_variables=["document"],
            template="è¯·ç”¨ä¸€å¥è¯æ¦‚æ‹¬ä»¥ä¸‹æ–‡æ¡£çš„æ ¸å¿ƒä¸»é¢˜å’Œä¸»è¦æ¦‚å¿µï¼š\n\n{document}"
        )
        
        response = self.llm.invoke(prompt.format(document=document[:2000]))
        return response.content
    
    def _extract_metadata(self, documents: List[str]) -> Dict[str, Any]:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            'total_documents': len(documents),
            'total_chunks': len(self.index_cache.get('original', {}).get('chunks', [])),
            'indexing_timestamp': datetime.now().isoformat(),
            'index_types': list(self.index_cache.keys())
        }
        return metadata
    
    def get_index_statistics(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for index_type, index_data in self.index_cache.items():
            if 'embeddings' in index_data:
                stats[index_type] = {
                    'vector_count': len(index_data['embeddings']),
                    'dimension': len(index_data['embeddings'][0]) if index_data['embeddings'] else 0
                }
        return stats
    
    def optimize_index(self, query_patterns: List[str]) -> None:
        """åŸºäºæŸ¥è¯¢æ¨¡å¼ä¼˜åŒ–ç´¢å¼•"""
        # åˆ†ææŸ¥è¯¢æ¨¡å¼ï¼Œè°ƒæ•´ç´¢å¼•ç­–ç•¥
        for pattern in query_patterns:
            if 'æ¯”è¾ƒ' in pattern or 'åˆ†æ' in pattern:
                # å¢åŠ æ‘˜è¦ç´¢å¼•çš„æƒé‡
                self._adjust_index_weights('summary', 1.5)
            elif 'æ­¥éª¤' in pattern or 'æ–¹æ³•' in pattern:
                # å¢åŠ åŸå§‹ç´¢å¼•çš„æƒé‡
                self._adjust_index_weights('original', 1.3)
    
    def _adjust_index_weights(self, index_type: str, weight: float):
        """è°ƒæ•´ç´¢å¼•æƒé‡"""
        if index_type in self.index_cache:
            # è¿™é‡Œå¯ä»¥å®ç°æƒé‡è°ƒæ•´é€»è¾‘
            print(f"è°ƒæ•´ {index_type} ç´¢å¼•æƒé‡ä¸º {weight}")
```

## 3 å¹³å°é›†æˆæ–¹æ¡ˆ

### 3.1 Notion é›†æˆ
```typescript
// Notion å¹³å°é€‚é…å™¨
class NotionAdapter implements PlatformAdapter {
    private notionClient: Client;
    private databaseId: string;
    
    constructor(integrationToken: string, databaseId: string) {
        this.notionClient = new Client({ auth: integrationToken });
        this.databaseId = databaseId;
    }
    
    async syncContent(): Promise<ContentItem[]> {
        const response = await this.notionClient.databases.query({
            database_id: this.databaseId,
            sorts: [{ property: 'Last edited time', direction: 'descending' }]
        });
        
        return response.results.map(page => ({
            id: page.id,
            title: this.extractTitle(page),
            content: this.extractContent(page),
            lastModified: page.last_edited_time,
            url: page.url
        }));
    }
    
    async createQAPlugin(): Promise<void> {
        // åˆ›å»º Notion æ’ä»¶é…ç½®
        const pluginConfig = {
            name: 'RAGå®æˆ˜ æ™ºèƒ½é—®ç­”',
            description: 'åŸºäº AI çš„çŸ¥è¯†æ£€ç´¢å’Œé—®ç­”æ’ä»¶',
            capabilities: ['query', 'search', 'suggest']
        };
        
        // æ³¨å†Œæ’ä»¶åˆ° Notion
        await this.registerPlugin(pluginConfig);
    }
    
    private extractTitle(page: any): string {
        // æå–é¡µé¢æ ‡é¢˜é€»è¾‘
        const titleProperty = page.properties.Title || page.properties.Name;
        return titleProperty?.title?.[0]?.plain_text || 'æ— æ ‡é¢˜';
    }
    
    private extractContent(page: any): string {
        // æå–é¡µé¢å†…å®¹é€»è¾‘
        // è¿™é‡Œéœ€è¦é€’å½’è·å–æ‰€æœ‰å—å†…å®¹
        return this.getPageContent(page.id);
    }
    
    private async getPageContent(pageId: string): Promise<string> {
        const blocks = await this.notionClient.blocks.children.list({ block_id: pageId });
        let content = '';
        
        for (const block of blocks.results) {
            if (block.type === 'paragraph') {
                content += block.paragraph.rich_text.map(text => text.plain_text).join('') + '\n';
            } else if (block.type === 'heading_1' || block.type === 'heading_2') {
                content += block[block.type].rich_text.map(text => text.plain_text).join('') + '\n';
            }
        }
        
        return content;
    }
}
```

### 3.2 Confluence é›†æˆ
```java
// Confluence å¹³å°é€‚é…å™¨
public class ConfluenceAdapter implements PlatformAdapter {
    private final ConfluenceClient confluenceClient;
    private final String spaceKey;
    
    public ConfluenceAdapter(String baseUrl, String username, String apiToken, String spaceKey) {
        this.confluenceClient = new ConfluenceClient(baseUrl, username, apiToken);
        this.spaceKey = spaceKey;
    }
    
    @Override
    public List<ContentItem> syncContent() throws Exception {
        List<ContentItem> contentItems = new ArrayList<>();
        
        // è·å–ç©ºé—´ä¸­çš„æ‰€æœ‰é¡µé¢
        PageResults pageResults = confluenceClient.getContentClient()
            .getPages(spaceKey, 0, 100, "page");
        
        for (Page page : pageResults.getResults()) {
            ContentItem item = new ContentItem();
            item.setId(page.getId());
            item.setTitle(page.getTitle());
            item.setContent(extractPageContent(page.getId()));
            item.setLastModified(page.getVersion().getWhen());
            item.setUrl(page.getLinks().getWebui());
            
            contentItems.add(item);
        }
        
        return contentItems;
    }
    
    @Override
    public void createQAPlugin() throws Exception {
        // åˆ›å»º Confluence å®
        Macro macro = new Macro();
        macro.setName("RAGå®æˆ˜ æ™ºèƒ½é—®ç­”");
        macro.setDescription("AI é©±åŠ¨çš„çŸ¥è¯†æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½");
        
        // æ³¨å†Œå®åˆ° Confluence
        confluenceClient.getContentClient().createMacro(macro);
    }
    
    private String extractPageContent(String pageId) throws Exception {
        // è·å–é¡µé¢å†…å®¹
        Page page = confluenceClient.getContentClient().getPage(pageId);
        return page.getBody().getStorage().getValue();
    }
}
```

### 3.3 é£ä¹¦å¤šç»´è¡¨é›†æˆ
```typescript
// é£ä¹¦å¤šç»´è¡¨å¹³å°é€‚é…å™¨
class FeishuAdapter implements PlatformAdapter {
    private appId: string;
    private appSecret: string;
    private tableId: string;
    
    constructor(appId: string, appSecret: string, tableId: string) {
        this.appId = appId;
        this.appSecret = appSecret;
        this.tableId = tableId;
    }
    
    async syncContent(): Promise<ContentItem[]> {
        const accessToken = await this.getAccessToken();
        
        // è·å–å¤šç»´è¡¨æ•°æ®
        const response = await fetch(`https://open.feishu.cn/open-apis/bitable/v1/apps/${this.tableId}/tables`, {
            headers: {
                'Authorization': `Bearer ${accessToken}`,
                'Content-Type': 'application/json'
            }
        });
        
        const data = await response.json();
        return this.transformTableData(data);
    }
    
    async createQAPlugin(): Promise<void> {
        // åˆ›å»ºé£ä¹¦æœºå™¨äºº
        const botConfig = {
            app_id: this.appId,
            app_secret: this.appSecret,
            features: ['chat', 'search', 'qa']
        };
        
        // æ³¨å†Œæœºå™¨äººåˆ°é£ä¹¦
        await this.registerBot(botConfig);
    }
    
    private async getAccessToken(): Promise<string> {
        const response = await fetch('https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                app_id: this.appId,
                app_secret: this.appSecret
            })
        });
        
        const data = await response.json();
        return data.tenant_access_token;
    }
    
    private transformTableData(tableData: any): ContentItem[] {
        // è½¬æ¢å¤šç»´è¡¨æ•°æ®ä¸ºå†…å®¹é¡¹
        return tableData.data.items.map(item => ({
            id: item.record_id,
            title: item.fields.Title || 'æ— æ ‡é¢˜',
            content: this.extractContentFromFields(item.fields),
            lastModified: new Date(item.record_id),
            url: `https://feishu.cn/base/${this.tableId}`
        }));
    }
    
    private extractContentFromFields(fields: any): string {
        // ä»å­—æ®µä¸­æå–å†…å®¹
        let content = '';
        for (const [key, value] of Object.entries(fields)) {
            if (typeof value === 'string' && value.length > 0) {
                content += `${key}: ${value}\n`;
            }
        }
        return content;
    }
}
```

## 4 å¼€å‘è·¯çº¿å›¾

### 4.1 ç¬¬ä¸€é˜¶æ®µï¼šæ ¸å¿ƒ RAG å¼•æ“ (2-3 å‘¨)
åŸºäº notebooks ç›®å½•ä¸­çš„ RAG æŠ€æœ¯å®ç°ï¼Œæ„å»ºå®Œæ•´çš„æ ¸å¿ƒå¼•æ“ï¼š

- **Week 1**: æ•´åˆ [1]_rag_setup_overview_chinese.py çš„ç¯å¢ƒè®¾ç½®å’ŒåŸºç¡€é…ç½®
  - å®ç°ç¯å¢ƒå˜é‡ç®¡ç†å’Œ API å¯†é’¥é…ç½®
  - é›†æˆ LangSmith è¿½è¸ªç³»ç»Ÿ
  - å»ºç«‹ Pinecone å’Œ ChromaDB å‘é‡æ•°æ®åº“è¿æ¥

- **Week 2**: é›†æˆ [2]_rag_with_multi_query_chinese.py çš„å¤šæŸ¥è¯¢è½¬æ¢æŠ€æœ¯
  - å®ç°åŠ¨æ€æŸ¥è¯¢ç”Ÿæˆç®—æ³•
  - å¼€å‘æ™ºèƒ½æ–‡æ¡£å»é‡æœºåˆ¶
  - æ„å»ºæŸ¥è¯¢ç­–ç•¥ä¼˜åŒ–å™¨

- **Week 3**: æ•´åˆ [3]_rag_routing_and_query_construction_chinese.py çš„æ™ºèƒ½è·¯ç”±
  - å®ç°æŸ¥è¯¢ç±»å‹è‡ªåŠ¨åˆ†ç±»
  - å¼€å‘ç­–ç•¥è·¯ç”±å¼•æ“
  - å»ºç«‹è·¯ç”±å†³ç­–æ—¥å¿—ç³»ç»Ÿ

### 4.2 ç¬¬äºŒé˜¶æ®µï¼šé«˜çº§ç´¢å¼•å’Œæ£€ç´¢ (2-3 å‘¨)
åŸºäº notebooks ä¸­çš„é«˜çº§æŠ€æœ¯ï¼Œå®ç°ä¼ä¸šçº§ç´¢å¼•å’Œæ£€ç´¢åŠŸèƒ½ï¼š

- **Week 1**: æ•´åˆ [4]_rag_indexing_and_advanced_retrieval_chinese.py çš„é«˜çº§ç´¢å¼•
  - å®ç°å¤šè¡¨ç¤ºç´¢å¼•ç³»ç»Ÿ
  - å¼€å‘æ‘˜è¦å’Œå…³é”®è¯ç´¢å¼•
  - æ„å»ºè¯­ä¹‰ç´¢å¼•å¼•æ“

- **Week 2**: é›†æˆ [5]_rag_retrieval_and_reranking_chinese.py çš„é‡æ–°æ’åº
  - å®ç° Cohere é‡æ–°æ’åºé›†æˆ
  - å¼€å‘å¤šçº§æ£€ç´¢ç­–ç•¥
  - æ„å»ºæ£€ç´¢è´¨é‡è¯„ä¼°ç³»ç»Ÿ

- **Week 3**: ç³»ç»Ÿé›†æˆå’Œä¼˜åŒ–
  - æ•´åˆæ‰€æœ‰ RAG ç»„ä»¶
  - å®ç°æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–
  - å»ºç«‹å®Œæ•´çš„æµ‹è¯•æ¡†æ¶

### 4.3 ç¬¬ä¸‰é˜¶æ®µï¼šå¹³å°é€‚é…å™¨ (3-4 å‘¨)
å¼€å‘å„å¹³å°çš„é›†æˆæ¨¡å—ï¼Œå®ç°å†…å®¹åŒæ­¥å’Œæ’ä»¶åŠŸèƒ½ï¼š

- **Week 1-2**: Notion é›†æˆæ¨¡å—
  - å®ç° Notion API é›†æˆ
  - å¼€å‘å†…å®¹åŒæ­¥å¼•æ“
  - æ„å»ºæ’ä»¶é…ç½®ç•Œé¢

- **Week 3-4**: Confluence å’Œé£ä¹¦å¤šç»´è¡¨é›†æˆ
  - å®ç° Confluence å®å’Œ API é›†æˆ
  - å¼€å‘é£ä¹¦æœºå™¨äººåŠŸèƒ½
  - å»ºç«‹ç»Ÿä¸€çš„å†…å®¹åŒæ­¥æ¥å£

### 4.4 ç¬¬å››é˜¶æ®µï¼šä½ä»£ç ç•Œé¢å’Œç”¨æˆ·ä½“éªŒ (2-3 å‘¨)
å¼€å‘ç”¨æˆ·å‹å¥½çš„é…ç½®å’Œç®¡ç†ç•Œé¢ï¼š

- **Week 1**: é…ç½®ç®¡ç†ç•Œé¢
  - å®ç°æ’ä»¶å®‰è£…å‘å¯¼
  - å¼€å‘è®¾ç½®ç®¡ç†é¢æ¿
  - æ„å»ºç”¨æˆ·æƒé™ç³»ç»Ÿ

- **Week 2**: ç›‘æ§å’Œè¿ç»´
  - é›†æˆ Prometheus å’Œ Grafana
  - å®ç°æ—¥å¿—æ”¶é›†å’Œåˆ†æ
  - å»ºç«‹å‘Šè­¦å’Œé€šçŸ¥ç³»ç»Ÿ

- **Week 3**: ç”¨æˆ·æ–‡æ¡£å’ŒåŸ¹è®­
  - åˆ›å»ºè¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£
  - å¼€å‘äº¤äº’å¼æ•™ç¨‹
  - å»ºç«‹ç”¨æˆ·æ”¯æŒç³»ç»Ÿ

### 4.5 ç¬¬äº”é˜¶æ®µï¼šæµ‹è¯•å’Œä¼˜åŒ– (2-3 å‘¨)
å…¨é¢çš„è´¨é‡ä¿è¯å’Œæ€§èƒ½ä¼˜åŒ–ï¼š

- **Week 1**: åŠŸèƒ½æµ‹è¯•
  - ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
  - API æ¥å£æµ‹è¯•
  - å¹³å°å…¼å®¹æ€§æµ‹è¯•

- **Week 2**: æ€§èƒ½æµ‹è¯•
  - è´Ÿè½½æµ‹è¯•å’Œå‹åŠ›æµ‹è¯•
  - å“åº”æ—¶é—´ä¼˜åŒ–
  - èµ„æºä½¿ç”¨ä¼˜åŒ–

- **Week 3**: éƒ¨ç½²å’Œè¿ç»´
  - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
  - ç›‘æ§ç³»ç»Ÿä¸Šçº¿
  - ç”¨æˆ·åŸ¹è®­å’Œæ”¯æŒ

## 5 æŠ€æœ¯æ ˆé€‰æ‹©

### 5.1 åç«¯æŠ€æœ¯
- **æ¡†æ¶**ï¼šFastAPI (Python 3.9+)
- **æ•°æ®åº“**ï¼šPostgreSQL + Redis
- **ä»»åŠ¡é˜Ÿåˆ—**ï¼šCelery + Redis
- **å®¹å™¨åŒ–**ï¼šDocker + Docker Compose

### 5.2 å‰ç«¯æŠ€æœ¯
- **æ¡†æ¶**ï¼šReact 18 + TypeScript
- **UI åº“**ï¼šAnt Design
- **çŠ¶æ€ç®¡ç†**ï¼šZustand
- **æ„å»ºå·¥å…·**ï¼šVite

### 5.3 AI/ML æŠ€æœ¯
- **RAG æ¡†æ¶**ï¼šLangChain
- **å‘é‡æ•°æ®åº“**ï¼šPinecone + ChromaDB
- **åµŒå…¥æ¨¡å‹**ï¼šOpenAI text-embedding-3-large
- **LLM æ¨¡å‹**ï¼šOpenAI GPT-3.5-turbo/GPT-4

### 5.4 äº‘æœåŠ¡å’Œè¿ç»´
- **äº‘å¹³å°**ï¼šAWS/Azure/GCP
- **å®¹å™¨ç¼–æ’**ï¼šKubernetes
- **CI/CD**ï¼šGitHub Actions
- **ç›‘æ§**ï¼šPrometheus + Grafana
- **æ—¥å¿—**ï¼šELK Stack

## 6 é¡¹ç›®ç»„ç»‡ç»“æ„

### 6.1 å¼€å‘å›¢é˜Ÿ
- **é¡¹ç›®ç»ç†**ï¼š1 äººï¼Œè´Ÿè´£é¡¹ç›®è§„åˆ’å’Œè¿›åº¦ç®¡ç†
- **åç«¯å¼€å‘**ï¼š2 äººï¼Œè´Ÿè´£ RAG å¼•æ“å’Œ API å¼€å‘
- **å‰ç«¯å¼€å‘**ï¼š2 äººï¼Œè´Ÿè´£ç”¨æˆ·ç•Œé¢å’Œå¹³å°é›†æˆ
- **AI å·¥ç¨‹å¸ˆ**ï¼š1 äººï¼Œè´Ÿè´£ RAG ç®—æ³•ä¼˜åŒ–
- **æµ‹è¯•å·¥ç¨‹å¸ˆ**ï¼š1 äººï¼Œè´Ÿè´£è´¨é‡ä¿è¯

### 6.2 ä»£ç ä»“åº“ç»“æ„
```
rag-practice-plugin/
â”œâ”€â”€ backend/                 # åç«¯æœåŠ¡
â”‚   â”œâ”€â”€ rag_engine/         # RAG æ ¸å¿ƒå¼•æ“
â”‚   â”œâ”€â”€ platform_adapters/  # å¹³å°é€‚é…å™¨
â”‚   â”œâ”€â”€ api/                # API æ¥å£
â”‚   â””â”€â”€ tests/              # åç«¯æµ‹è¯•
â”œâ”€â”€ frontend/               # å‰ç«¯åº”ç”¨
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ platform_integrations/  # å¹³å°é›†æˆä»£ç 
â”‚   â”œâ”€â”€ notion/
â”‚   â”œâ”€â”€ confluence/
â”‚   â””â”€â”€ feishu/
â”œâ”€â”€ docs/                   # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ docker/                 # Docker é…ç½®
â””â”€â”€ scripts/                # éƒ¨ç½²è„šæœ¬
```

## 7 é£é™©è¯„ä¼°ä¸åº”å¯¹

### 7.1 æŠ€æœ¯é£é™©
- **RAG æ€§èƒ½é—®é¢˜**ï¼šé€šè¿‡å¤šçº§ç¼“å­˜å’Œå¼‚æ­¥å¤„ç†ä¼˜åŒ–
- **å¹³å° API é™åˆ¶**ï¼šå®ç°æ™ºèƒ½é™æµå’Œé‡è¯•æœºåˆ¶
- **å‘é‡æ•°æ®åº“æ‰©å±•æ€§**ï¼šé‡‡ç”¨åˆ†å¸ƒå¼æ¶æ„å’Œåˆ†ç‰‡ç­–ç•¥

### 7.2 ä¸šåŠ¡é£é™©
- **å¹³å°æ”¿ç­–å˜åŒ–**ï¼šä¿æŒä¸å¹³å°æ–¹çš„æ²Ÿé€šï¼ŒåŠæ—¶è°ƒæ•´ç­–ç•¥
- **ç”¨æˆ·æ¥å—åº¦**ï¼šé€šè¿‡ç”¨æˆ·è°ƒç ”å’Œè¿­ä»£ä¼˜åŒ–æå‡ä½“éªŒ
- **ç«äº‰å‹åŠ›**ï¼šæŒç»­æŠ€æœ¯åˆ›æ–°å’Œå·®å¼‚åŒ–åŠŸèƒ½å¼€å‘

### 7.3 è¿è¥é£é™©
- **æ•°æ®å®‰å…¨**ï¼šå®æ–½ä¸¥æ ¼çš„æ•°æ®åŠ å¯†å’Œè®¿é—®æ§åˆ¶
- **æœåŠ¡ç¨³å®šæ€§**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§å’Œæ•…éšœæ¢å¤æœºåˆ¶
- **æˆæœ¬æ§åˆ¶**ï¼šä¼˜åŒ– API è°ƒç”¨ç­–ç•¥ï¼Œæ§åˆ¶è¿è¥æˆæœ¬

## 8 æˆåŠŸæŒ‡æ ‡

### 8.1 æŠ€æœ¯æŒ‡æ ‡
- **å“åº”æ—¶é—´**ï¼šå¹³å‡æŸ¥è¯¢å“åº”æ—¶é—´ < 2 ç§’
- **å‡†ç¡®ç‡**ï¼šRAG å›ç­”å‡†ç¡®ç‡ > 85%
- **å¯ç”¨æ€§**ï¼šæœåŠ¡å¯ç”¨æ€§ > 99.5%
- **æ‰©å±•æ€§**ï¼šæ”¯æŒå¹¶å‘ç”¨æˆ·æ•° > 1000

### 8.2 ä¸šåŠ¡æŒ‡æ ‡
- **ç”¨æˆ·å¢é•¿**ï¼šæœˆæ´»è·ƒç”¨æˆ·å¢é•¿ç‡ > 20%
- **å¹³å°è¦†ç›–**ï¼šæ”¯æŒå¹³å°æ•°é‡ > 5 ä¸ª
- **ç”¨æˆ·æ»¡æ„åº¦**ï¼šNPS è¯„åˆ† > 50
- **å•†ä¸šåŒ–**ï¼šä»˜è´¹ç”¨æˆ·è½¬åŒ–ç‡ > 5%

## 9 æ€»ç»“

æœ¬é¡¹ç›®é€šè¿‡æ•´åˆ RAGå®æˆ˜ çš„æ ¸å¿ƒæŠ€æœ¯å’Œä½ä»£ç çŸ¥è¯†æ’ä»¶æ¦‚å¿µï¼Œä¸ºåä½œå¹³å°æä¾›æ™ºèƒ½åŒ–çš„çŸ¥è¯†ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚åŸºäºç°æœ‰ notebooks ä¸­çš„ RAG å®ç°ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å¯æ‰©å±•çš„æ’ä»¶ç”Ÿæ€ç³»ç»Ÿï¼Œå®ç°çŸ¥è¯†æ£€ç´¢çš„æ™ºèƒ½åŒ–å’Œå¹³å°é›†æˆçš„æ ‡å‡†åŒ–ã€‚

é€šè¿‡åˆ†é˜¶æ®µçš„å¼€å‘ç­–ç•¥å’Œå…¨é¢çš„æŠ€æœ¯æ¶æ„è®¾è®¡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶é¡¹ç›®é£é™©ï¼Œç¡®ä¿æŒ‰æ—¶äº¤ä»˜é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œé¡¹ç›®çš„æˆåŠŸå®æ–½å°†ä¸ºå›¢é˜Ÿç§¯ç´¯å®è´µçš„ RAG æŠ€æœ¯ç»éªŒå’Œå¹³å°é›†æˆç»éªŒï¼Œä¸ºæœªæ¥çš„æŠ€æœ¯å‘å±•å¥ å®šåšå®åŸºç¡€ã€‚

## 10 å…·ä½“å®ç°ç¤ºä¾‹

### 10.1 å¿«é€Ÿå¯åŠ¨è„šæœ¬
ä¸ºäº†å¸®åŠ©å¼€å‘è€…å¿«é€Ÿä¸Šæ‰‹ï¼Œæˆ‘ä»¬æä¾›å®Œæ•´çš„é¡¹ç›®å¯åŠ¨è„šæœ¬ï¼š

```bash
#!/bin/bash
# å¿«é€Ÿå¯åŠ¨è„šæœ¬ - RAGå®æˆ˜ ä½ä»£ç çŸ¥è¯†æ’ä»¶

echo "ğŸš€ å¯åŠ¨ RAGå®æˆ˜ ä½ä»£ç çŸ¥è¯†æ’ä»¶å¼€å‘ç¯å¢ƒ..."

# æ£€æŸ¥ Python ç‰ˆæœ¬
python_version=$(python3 --version 2>&1 | awk '{print $2}' | cut -d. -f1,2)
required_version="3.9"

if [ "$(printf '%s\n' "$required_version" "$python_version" | sort -V | head -n1)" != "$required_version" ]; then
    echo "âŒ éœ€è¦ Python 3.9+ ç‰ˆæœ¬ï¼Œå½“å‰ç‰ˆæœ¬: $python_version"
    exit 1
fi

echo "âœ… Python ç‰ˆæœ¬æ£€æŸ¥é€šè¿‡: $python_version"

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
if [ ! -d "venv" ]; then
    echo "ğŸ“¦ åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ..."
    python3 -m venv venv
fi

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
echo "ğŸ”§ æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ..."
source venv/bin/activate

# å®‰è£…ä¾èµ–
echo "ğŸ“¥ å®‰è£…é¡¹ç›®ä¾èµ–..."
pip install -r requirements_chinese.txt

# è®¾ç½®ç¯å¢ƒå˜é‡
echo "âš™ï¸ é…ç½®ç¯å¢ƒå˜é‡..."
if [ ! -f ".env" ]; then
    cat > .env << EOF
# OpenAI API é…ç½®
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_API_BASE=https://api.openai.com/v1

# Pinecone é…ç½®
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_API_HOST=your_pinecone_host_here
PINECONE_INDEX_NAME=rag-practice-index

# Cohere é…ç½®
COHERE_API_KEY=your_cohere_api_key_here

# LangSmith é…ç½®
LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=
LANGCHAIN_API_KEY=

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://user:password@localhost:5432/rag_plugin
REDIS_URL=redis://localhost:6379/0

# åº”ç”¨é…ç½®
APP_ENV=development
DEBUG=true
LOG_LEVEL=INFO
EOF
    echo "ğŸ“ å·²åˆ›å»º .env æ–‡ä»¶ï¼Œè¯·å¡«å†™æ‚¨çš„ API å¯†é’¥"
fi

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
echo "ğŸŒ å¯åŠ¨å¼€å‘æœåŠ¡å™¨..."
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

echo "ğŸ‰ å¼€å‘ç¯å¢ƒå¯åŠ¨å®Œæˆï¼"
echo "ğŸ“– è®¿é—® http://localhost:8000/docs æŸ¥çœ‹ API æ–‡æ¡£"
echo "ğŸ”§ è®¿é—® http://localhost:8000/admin è¿›å…¥ç®¡ç†ç•Œé¢"
```

### 10.2 Docker éƒ¨ç½²é…ç½®
ä¸ºäº†ç®€åŒ–éƒ¨ç½²æµç¨‹ï¼Œæˆ‘ä»¬æä¾›å®Œæ•´çš„ Docker é…ç½®ï¼š

```yaml
# docker-compose.yml
version: '3.8'

services:
  # ä¸»åº”ç”¨æœåŠ¡
  rag-plugin-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - APP_ENV=production
      - DATABASE_URL=postgresql://rag_user:rag_password@postgres:5432/rag_plugin
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - postgres
      - redis
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped
    networks:
      - rag-network

  # PostgreSQL æ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=rag_plugin
      - POSTGRES_USER=rag_user
      - POSTGRES_PASSWORD=rag_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - rag-network

  # Redis ç¼“å­˜
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - rag-network

  # Nginx åå‘ä»£ç†
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - rag-plugin-app
    restart: unless-stopped
    networks:
      - rag-network

  # Prometheus ç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - rag-network

  # Grafana å¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    networks:
      - rag-network

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  rag-network:
    driver: bridge
```

### 10.3 ç”Ÿäº§ç¯å¢ƒé…ç½®
é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒçš„ä¼˜åŒ–é…ç½®ï¼š

```python
# config/production.py
import os
from typing import Dict, Any

class ProductionConfig:
    """ç”Ÿäº§ç¯å¢ƒé…ç½®"""
    
    # åŸºç¡€é…ç½®
    DEBUG = False
    TESTING = False
    
    # å®‰å…¨é…ç½®
    SECRET_KEY = os.environ.get('SECRET_KEY', 'your-secret-key-here')
    ALLOWED_HOSTS = os.environ.get('ALLOWED_HOSTS', 'localhost,127.0.0.1').split(',')
    
    # æ•°æ®åº“é…ç½®
    DATABASE_URL = os.environ.get('DATABASE_URL', 'postgresql://user:pass@localhost/db')
    REDIS_URL = os.environ.get('REDIS_URL', 'redis://localhost:6379/0')
    
    # RAG å¼•æ“é…ç½®
    RAG_CONFIG = {
        'max_concurrent_queries': 100,
        'query_timeout': 30,
        'cache_ttl': 3600,
        'enable_rate_limiting': True,
        'rate_limit_per_minute': 60,
        'enable_query_logging': True,
        'enable_performance_monitoring': True
    }
    
    # å‘é‡æ•°æ®åº“é…ç½®
    VECTOR_STORE_CONFIG = {
        'pinecone': {
            'api_key': os.environ.get('PINECONE_API_KEY'),
            'environment': os.environ.get('PINECONE_API_HOST'),
            'index_name': os.environ.get('PINECONE_INDEX_NAME'),
            'dimension': 3072,
            'metric': 'cosine'
        },
        'chromadb': {
            'persist_directory': '/app/data/chromadb',
            'anonymized_telemetry': False
        }
    }
    
    # ç¼“å­˜é…ç½®
    CACHE_CONFIG = {
        'default': 'redis',
        'redis': {
            'host': os.environ.get('REDIS_HOST', 'localhost'),
            'port': int(os.environ.get('REDIS_PORT', 6379)),
            'db': int(os.environ.get('REDIS_DB', 0)),
            'password': os.environ.get('REDIS_PASSWORD'),
            'max_connections': 20
        }
    }
    
    # æ—¥å¿—é…ç½®
    LOGGING_CONFIG = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'standard': {
                'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
            },
        },
        'handlers': {
            'default': {
                'level': 'INFO',
                'formatter': 'standard',
                'class': 'logging.StreamHandler',
            },
            'file': {
                'level': 'INFO',
                'formatter': 'standard',
                'class': 'logging.handlers.RotatingFileHandler',
                'filename': '/app/logs/app.log',
                'maxBytes': 10485760,  # 10MB
                'backupCount': 5
            }
        },
        'loggers': {
            '': {
                'handlers': ['default', 'file'],
                'level': 'INFO',
                'propagate': False
            }
        }
    }
    
    # ç›‘æ§é…ç½®
    MONITORING_CONFIG = {
        'prometheus': {
            'enabled': True,
            'port': 8001,
            'path': '/metrics'
        },
        'health_check': {
            'enabled': True,
            'path': '/health',
            'timeout': 5
        }
    }
    
    # å¹³å°é›†æˆé…ç½®
    PLATFORM_CONFIG = {
        'notion': {
            'max_pages_per_sync': 1000,
            'sync_interval': 3600,  # 1å°æ—¶
            'rate_limit': 100  # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
        },
        'confluence': {
            'max_pages_per_sync': 500,
            'sync_interval': 7200,  # 2å°æ—¶
            'rate_limit': 50
        },
        'feishu': {
            'max_records_per_sync': 2000,
            'sync_interval': 1800,  # 30åˆ†é’Ÿ
            'rate_limit': 200
        }
    }
    
    @classmethod
    def get_config(cls) -> Dict[str, Any]:
        """è·å–é…ç½®å­—å…¸"""
        return {
            key: value for key, value in cls.__dict__.items()
            if not key.startswith('_') and key.isupper()
        }
```

## 11 æœ€ä½³å®è·µæŒ‡å—

### 11.1 RAG æ€§èƒ½ä¼˜åŒ–
åŸºäº notebooks ä¸­çš„å®è·µç»éªŒï¼Œæˆ‘ä»¬æ€»ç»“ä»¥ä¸‹æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š

**æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥**ï¼š
- ä½¿ç”¨å¤šæŸ¥è¯¢è½¬æ¢æ—¶ï¼Œé™åˆ¶æŸ¥è¯¢æ•°é‡åœ¨ 3-5 ä¸ªä¹‹é—´
- å®ç°æ™ºèƒ½è·¯ç”±ï¼Œæ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©æœ€ä½³æ£€ç´¢ç­–ç•¥
- å¯ç”¨é‡æ–°æ’åºåŠŸèƒ½ï¼Œä½†è®¾ç½®åˆç†çš„ top_n å€¼ï¼ˆé€šå¸¸ä¸º 3-5ï¼‰

**ç´¢å¼•ä¼˜åŒ–ç­–ç•¥**ï¼š
- æ–‡æ¡£åˆ†å—å¤§å°æ§åˆ¶åœ¨ 800-1500 å­—ç¬¦ä¹‹é—´
- åˆ†å—é‡å è®¾ç½®ä¸ºåˆ†å—å¤§å°çš„ 15-20%
- ä½¿ç”¨å¤šè¡¨ç¤ºç´¢å¼•ï¼Œä½†é¿å…è¿‡åº¦ç´¢å¼•å¯¼è‡´å­˜å‚¨æˆæœ¬å¢åŠ 

**ç¼“å­˜ç­–ç•¥**ï¼š
- å®ç°å¤šçº§ç¼“å­˜ï¼šå†…å­˜ç¼“å­˜ + Redis ç¼“å­˜ + å‘é‡æ•°æ®åº“ç¼“å­˜
- ç¼“å­˜æŸ¥è¯¢ç»“æœå’ŒåµŒå…¥å‘é‡ï¼Œè®¾ç½®åˆç†çš„ TTL
- ä½¿ç”¨ LRU ç®—æ³•ç®¡ç†å†…å­˜ç¼“å­˜

### 11.2 å¹³å°é›†æˆæœ€ä½³å®è·µ
**API é™æµå¤„ç†**ï¼š
- å®ç°æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
- ç›‘æ§ API é…é¢ä½¿ç”¨æƒ…å†µï¼Œæå‰é¢„è­¦
- å®ç°è¯·æ±‚é˜Ÿåˆ—ï¼Œé¿å…è¶…å‡ºå¹³å°é™åˆ¶

**å†…å®¹åŒæ­¥ç­–ç•¥**ï¼š
- å¢é‡åŒæ­¥ä¼˜å…ˆäºå…¨é‡åŒæ­¥
- å®ç°å†…å®¹å˜æ›´æ£€æµ‹ï¼ŒåªåŒæ­¥ä¿®æ”¹çš„éƒ¨åˆ†
- è®¾ç½®åˆç†çš„åŒæ­¥é¢‘ç‡ï¼Œå¹³è¡¡å®æ—¶æ€§å’Œèµ„æºæ¶ˆè€—

**é”™è¯¯å¤„ç†**ï¼š
- å®ç°ä¼˜é›…é™çº§ï¼Œå½“å¹³å° API ä¸å¯ç”¨æ—¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜
- è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—ï¼Œä¾¿äºé—®é¢˜æ’æŸ¥
- æä¾›ç”¨æˆ·å‹å¥½çš„é”™è¯¯æç¤º

### 11.3 å®‰å…¨æœ€ä½³å®è·µ
**æ•°æ®å®‰å…¨**ï¼š
- æ‰€æœ‰æ•æ„Ÿæ•°æ®ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
- å®ç° API å¯†é’¥è½®æ¢æœºåˆ¶
- å¯¹ç”¨æˆ·æ•°æ®è¿›è¡ŒåŠ å¯†å­˜å‚¨

**è®¿é—®æ§åˆ¶**ï¼š
- å®ç°åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ï¼ˆRBACï¼‰
- ä½¿ç”¨ JWT ä»¤ç‰Œè¿›è¡Œèº«ä»½éªŒè¯
- å®ç° API è®¿é—®é¢‘ç‡é™åˆ¶

**å®¡è®¡æ—¥å¿—**ï¼š
- è®°å½•æ‰€æœ‰ç”¨æˆ·æ“ä½œå’Œç³»ç»Ÿäº‹ä»¶
- å®ç°æ—¥å¿—å®Œæ•´æ€§ä¿æŠ¤
- å®šæœŸå®¡æŸ¥è®¿é—®æ—¥å¿—

## 12 æ•…éšœæ’é™¤æŒ‡å—

### 12.1 å¸¸è§é—®é¢˜è§£å†³

**RAG æŸ¥è¯¢å“åº”æ…¢**ï¼š
1. æ£€æŸ¥å‘é‡æ•°æ®åº“è¿æ¥çŠ¶æ€
2. éªŒè¯åµŒå…¥æ¨¡å‹ API å“åº”æ—¶é—´
3. æ£€æŸ¥æ–‡æ¡£ç´¢å¼•æ˜¯å¦å®Œæ•´
4. ä¼˜åŒ–æŸ¥è¯¢ç­–ç•¥å’Œåˆ†å—å¤§å°

**å¹³å°åŒæ­¥å¤±è´¥**ï¼š
1. éªŒè¯ API å¯†é’¥å’Œæƒé™
2. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®
3. æŸ¥çœ‹å¹³å° API çŠ¶æ€é¡µé¢
4. æ£€æŸ¥è¯·æ±‚é¢‘ç‡æ˜¯å¦è¶…é™

**å†…å­˜ä½¿ç”¨è¿‡é«˜**ï¼š
1. æ£€æŸ¥ç¼“å­˜é…ç½®å’Œ TTL è®¾ç½®
2. ä¼˜åŒ–æ–‡æ¡£åˆ†å—ç­–ç•¥
3. å®ç°å†…å­˜ä½¿ç”¨ç›‘æ§
4. è€ƒè™‘ä½¿ç”¨æµå¼å¤„ç†

### 12.2 ç›‘æ§æŒ‡æ ‡
**å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆKPIï¼‰**ï¼š
- æŸ¥è¯¢å“åº”æ—¶é—´ï¼ˆP50, P95, P99ï¼‰
- æŸ¥è¯¢æˆåŠŸç‡
- ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡ï¼ˆCPU, å†…å­˜, ç£ç›˜ï¼‰
- API è°ƒç”¨é¢‘ç‡å’ŒæˆåŠŸç‡

**ä¸šåŠ¡æŒ‡æ ‡**ï¼š
- æ´»è·ƒç”¨æˆ·æ•°
- æŸ¥è¯¢æ•°é‡å’Œè´¨é‡
- å¹³å°é›†æˆçŠ¶æ€
- ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†

## 13 æœªæ¥æ‰©å±•è®¡åˆ’

### 13.1 æŠ€æœ¯æ‰©å±•
**å¤šæ¨¡æ€æ”¯æŒ**ï¼š
- é›†æˆå›¾åƒå’Œè§†é¢‘ç†è§£èƒ½åŠ›
- æ”¯æŒéŸ³é¢‘å†…å®¹æ£€ç´¢
- å®ç°è·¨æ¨¡æ€çŸ¥è¯†å…³è”

**é«˜çº§ AI åŠŸèƒ½**ï¼š
- é›†æˆå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ
- å®ç°ä¸ªæ€§åŒ–çŸ¥è¯†æ¨è
- æ”¯æŒå¤šè¯­è¨€å†…å®¹å¤„ç†

**è¾¹ç¼˜è®¡ç®—**ï¼š
- æ”¯æŒæœ¬åœ°éƒ¨ç½²å’Œç¦»çº¿ä½¿ç”¨
- å®ç°è¾¹ç¼˜èŠ‚ç‚¹åŒæ­¥
- ä¼˜åŒ–ç§»åŠ¨ç«¯æ€§èƒ½

### 13.2 å¹³å°æ‰©å±•
**æ–°å¢å¹³å°æ”¯æŒ**ï¼š
- Microsoft Teams é›†æˆ
- Slack æœºå™¨äººæ”¯æŒ
- ä¼ä¸šå¾®ä¿¡é›†æˆ
- é’‰é’‰é›†æˆ

**å‚ç›´è¡Œä¸šé€‚é…**ï¼š
- é‡‘èè¡Œä¸šåˆè§„æ£€æŸ¥
- åŒ»ç–—è¡Œä¸šçŸ¥è¯†ç®¡ç†
- æ•™è‚²è¡Œä¸šå†…å®¹æ¨è
- åˆ¶é€ ä¸šæŠ€æœ¯æ–‡æ¡£ç®¡ç†

## 14 æ€»ç»“ä¸å±•æœ›

æœ¬é¡¹ç›®é€šè¿‡æ•´åˆ RAGå®æˆ˜ çš„æ ¸å¿ƒæŠ€æœ¯å’Œä½ä»£ç çŸ¥è¯†æ’ä»¶æ¦‚å¿µï¼Œä¸ºåä½œå¹³å°æä¾›æ™ºèƒ½åŒ–çš„çŸ¥è¯†ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚åŸºäºç°æœ‰ notebooks ä¸­çš„ RAG å®ç°ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ã€å¯æ‰©å±•çš„æ’ä»¶ç”Ÿæ€ç³»ç»Ÿã€‚

**é¡¹ç›®äº®ç‚¹**ï¼š
- åŸºäºæˆç†Ÿçš„ RAG æŠ€æœ¯æ ˆï¼Œç¡®ä¿æŠ€æœ¯å¯é æ€§
- æ”¯æŒå¤šç§ä¸»æµåä½œå¹³å°ï¼Œæä¾›ç»Ÿä¸€ä½“éªŒ
- ä½ä»£ç é›†æˆæ–¹å¼ï¼Œé™ä½ç”¨æˆ·ä½¿ç”¨é—¨æ§›
- å®Œæ•´çš„ç›‘æ§å’Œè¿ç»´ä½“ç³»ï¼Œä¿éšœæœåŠ¡ç¨³å®šæ€§

**æŠ€æœ¯ä»·å€¼**ï¼š
- ç§¯ç´¯äº†ä¸°å¯Œçš„ RAG æŠ€æœ¯å®è·µç»éªŒ
- å»ºç«‹äº†å¯å¤ç”¨çš„å¹³å°é›†æˆæ¡†æ¶
- å½¢æˆäº†å®Œæ•´çš„ AI åº”ç”¨å¼€å‘æµç¨‹
- ä¸ºå›¢é˜ŸæŠ€æœ¯èƒ½åŠ›æå‡å¥ å®šåŸºç¡€

**å•†ä¸šä»·å€¼**ï¼š
- è§£å†³äº†ä¼ä¸šçŸ¥è¯†ç®¡ç†çš„ç—›ç‚¹é—®é¢˜
- æä¾›äº†å·®å¼‚åŒ–çš„ç«äº‰ä¼˜åŠ¿
- å»ºç«‹äº†å¯æŒç»­çš„æŠ€æœ¯æœåŠ¡æ¨¡å¼
- ä¸ºæœªæ¥äº§å“åŒ–å¥ å®šåŸºç¡€

é€šè¿‡åˆ†é˜¶æ®µçš„å¼€å‘ç­–ç•¥å’Œå…¨é¢çš„æŠ€æœ¯æ¶æ„è®¾è®¡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶é¡¹ç›®é£é™©ï¼Œç¡®ä¿æŒ‰æ—¶äº¤ä»˜é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œé¡¹ç›®çš„æˆåŠŸå®æ–½å°†ä¸ºå›¢é˜Ÿç§¯ç´¯å®è´µçš„ RAG æŠ€æœ¯ç»éªŒå’Œå¹³å°é›†æˆç»éªŒï¼Œä¸ºæœªæ¥çš„æŠ€æœ¯å‘å±•å¥ å®šåšå®åŸºç¡€ã€‚

å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä¼˜åŒ– RAG ç®—æ³•æ€§èƒ½ï¼Œæ‰©å±•å¹³å°æ”¯æŒèŒƒå›´ï¼Œå¹¶æ¢ç´¢æ›´å¤š AI æŠ€æœ¯çš„åº”ç”¨åœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ æ™ºèƒ½ã€é«˜æ•ˆçš„çŸ¥è¯†ç®¡ç†ä½“éªŒã€‚
